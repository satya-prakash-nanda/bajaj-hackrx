from typing import List
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

from .prompt import get_custom_prompt
from .config import settings

import logging

logger = logging.getLogger(__name__)


def query_pipeline(questions: List[str], vectorstore: FAISS) -> List[str]:
    """
    Runs the QA pipeline on a given vectorstore and list of questions using OpenAI.

    Args:
        questions (List[str]): List of user questions.
        vectorstore (FAISS): Vectorstore built from document chunks.

    Returns:
        List[str]: Answers generated by the LLM.
    """

    # Step 1: Set up retriever
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

    # Step 2: Prompt template
    prompt = get_custom_prompt()

    # Step 3: Build LLM (OpenAI, single API key from settings)
    llm = ChatOpenAI(
        model=settings.QA_MODEL_NAME,     
        temperature=0,
        openai_api_key=settings.OPENAI_API_KEY  # ‚úÖ fixed
    )

    # Step 4: Create RetrievalQA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}
    )

    # Step 5: Process each question
    answers = []
    for question in questions:
        try:
            logger.info(f"üîç Answering question: {question}")
            response = qa_chain.invoke(question)
            answers.append(response["result"].strip())
            logger.info("‚úÖ Answered successfully.")
        except Exception as e:
            logger.error(f"‚õî Failed to answer question '{question}': {e}")
            answers.append("Sorry, the model couldn't generate a response.")

    return answers
